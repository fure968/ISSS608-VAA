[
  {
    "objectID": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html",
    "href": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html",
    "title": "Hands-on 7",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\nIn this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages."
  },
  {
    "objectID": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html#plotting-multiple-calendar-heatmaps",
    "title": "Hands-on 7",
    "section": "5.1 Plotting Multiple Calendar Heatmaps",
    "text": "5.1 Plotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\ncount the number of attacks by country, calculate the percent of attackes by country, and save the results in a tibble data frame.\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e.¬†top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nPlotting Multiple Calendar Heatmaps Step 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html#plotting-cycle-plot",
    "href": "Hands-on_exercise/Hands-on_ex7/Hands-on_ex7.html#plotting-cycle-plot",
    "title": "Hands-on 7",
    "section": "5.2 Plotting Cycle Plot",
    "text": "5.2 Plotting Cycle Plot\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\nStep 1: Data Import For the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"arrivals_by_air.xlsx\")\n\nStep 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nStep 3: Extracting the target country Next, the code chunk below is use to extract data for the target country (i.e.¬†Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 4: Computing year average arrivals by month The code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\nSrep 5: Plotting the cycle plot The code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\nPlotting Slopegraph\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\nStep 1: Data Import\n\nrice &lt;- read_csv(\"rice.csv\")\n\nStep 2: Plotting the slopegraph Next, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Take-home/Take-home_ex3/Take-home_x03.html",
    "href": "Take-home/Take-home_ex3/Take-home_x03.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "xx"
  },
  {
    "objectID": "Take-home/Take-home_ex1/tkx01.html",
    "href": "Take-home/Take-home_ex1/tkx01.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "üìñ Introduction\nIn Singapore, the residential property market is bifurcated into public and private sectors. The public housing sector serves households with a monthly income of up to S$14,000, aiming to offer affordable housing solutions to the broader community. In contrast, the private housing sector caters to households with incomes surpassing S$14,000.\nFor this analysis, we will leverage REALIS transaction data covering the period from 1st January 2023 to 31st March 2024. Our exploration will be conducted using ggplot2 and its associated extensions.\n\n\nSetting the Scene\nIn this scenario, we assume the role of a graphical editor for a media company. We have been tasked with creating data visualizations to illuminate the private residential market and its sub-markets in Singapore for the first quarter of 2024.\n\n\nData Preparation\nLoad R Packages\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales,\n               viridis, cowplot, \n               dplyr, plotly,\n               tidyr, lubridate, \n               ggplot2, ggExtra)\n\nIdentify the missing values in the data set and removed any missing elements. ‚ÄòType of Sale‚Äô and ‚ÄòProperty type‚Äô has been converted to factor format. ‚ÄòTransacted Price ($)‚Äô and ‚ÄòArea (SQFT)‚Äô are converted to numeric data types. ‚ÄòType of Sale‚Äô has been group into three categories. ‚ÄòSale Date‚Äô has been converted to Date format.\nThe process had been repeated for all five data sets.\n\nImport Data SetsConverting DataConverting Dates\n\n\n\nds1 &lt;- read_csv(\"data/ds1.csv\")\nds2 &lt;- read_csv(\"data/ds2.csv\")\nds3 &lt;- read_csv(\"data/ds3.csv\")\nds4 &lt;- read_csv(\"data/ds4.csv\")\nds5 &lt;- read_csv(\"data/ds5.csv\")\n\n\n\n\nprepare_dataset &lt;- function(ds) {\n  colSums(is.na(ds))\n  ds &lt;- na.omit(ds)\n  \n  ds$`Type of Sale` &lt;- tolower(as.character(ds$`Type of Sale`))\n  ds$`Type of Sale` &lt;- ifelse(ds$`Type of Sale` %in% c(\"new sale\", \"resale\"), ds$`Type of Sale`, \"other\")\n  ds$`Type of Sale` &lt;- as.factor(ds$`Type of Sale`)\n  \n  ds$`Property Type` &lt;- as.factor(ds$`Property Type`)\n  \n  ds$`Transacted Price ($)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Transacted Price ($)`, perl = TRUE))\n  ds$`Area (SQFT)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Area (SQFT)`, perl = TRUE))\n  ds$`Unit Price ($ PSF)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Unit Price ($ PSF)`, perl = TRUE))\n  \n  return(ds)\n}\n\n# Apply the function to each dataset\nds1 &lt;- prepare_dataset(ds1)\nds2 &lt;- prepare_dataset(ds2)\nds3 &lt;- prepare_dataset(ds3)\nds4 &lt;- prepare_dataset(ds4)\nds5 &lt;- prepare_dataset(ds5)\n\n# Combine the datasets\ncombined_ds &lt;- rbind(ds1, ds2, ds3, ds4, ds5)\n\n\n\n\n# Convert Sale Date to Date format\nds1$`Sale Date` &lt;- dmy(ds1$`Sale Date`)\nds2$`Sale Date` &lt;- dmy(ds2$`Sale Date`)\nds3$`Sale Date` &lt;- dmy(ds3$`Sale Date`)\nds4$`Sale Date` &lt;- dmy(ds4$`Sale Date`)\nds5$`Sale Date` &lt;- dmy(ds5$`Sale Date`)\n\n\n\n\n\n\nEDA\nIn this section, we will visualize the relationships between Property Types vs.¬†Planning Region and Sale trend from Jan 2023 - Mar 2024.\n\nTransactions by Regions and Property TypeTransactions by MonthPurchaser Address IndicatorSale Categories vs.¬†Property Types\n\n\n\n\nClick to show code\n\n\ntransactions_heatmap &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`, `Property Type`) %&gt;%\n  summarise(Transactions = n()) %&gt;%\n  ungroup()\n\nheatmap1 &lt;- ggplot(transactions_heatmap, aes(x = `Planning Region`, y = `Property Type`, fill = Transactions)) +\n  geom_tile() +\n  labs(title = \"Heatmap of Transactions by Region and Property Type\",\n       x = \"Region\",\n       y = \"Property Type\") +\n  scale_fill_viridis_c()\n\n\n\n\n\n\nClick to show code\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Combine all datasets into one dataframe\ncombined_ds &lt;- bind_rows(ds1, ds2, ds3, ds4, ds5)\n\n# Pre-process the combined dataset\ncombined_ds$`Sale Date` &lt;- as.Date(combined_ds$`Sale Date`, format = \"%Y-%m-%d\")\ncombined_ds$Year &lt;- year(combined_ds$`Sale Date`)\ncombined_ds$Month &lt;- factor(format(combined_ds$`Sale Date`, \"%b\"), levels = month.abb)\ncombined_ds$`Number of Units` &lt;- as.numeric(combined_ds$`Number of Units`)\n\n# Summarize data to get total units sold by month and year\nmonthly_sales_combined &lt;- combined_ds %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(Units_Sold = sum(`Number of Units`, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Month = factor(Month, levels = month.abb)) # Ensure months are in the correct order\n\n# Create a single bar chart with month and year\nbarchart1 &lt;- ggplot(monthly_sales_combined, aes(x = Month, y = Units_Sold, fill = as.factor(Year))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_x_discrete(drop = FALSE) + # Ensures all months are shown\n  labs(title = \"Total Number of Units Sold by Month and Year\",\n       x = \"Month\",\n       y = \"Number of Units Sold\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\nClick to show code\n\n\npercentage_data &lt;- combined_ds %&gt;%\n  group_by(`Property Type`, `Purchaser Address Indicator`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  group_by(`Property Type`) %&gt;%\n  mutate(Percentage = (Count / sum(Count)) * 100) %&gt;%\n  ungroup()\nplot &lt;- ggplot(percentage_data, aes(x = `Property Type`, y = Percentage, fill = `Purchaser Address Indicator`)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Percentage)), \n            position = position_stack(vjust = 0.5), \n            size = 3, \n            color = \"black\") +\n  labs(\n    title = \"Percentage of Purchaser Address Indicator by Property Type\",\n    x = \"Property Type\",\n    y = \"Percentage (%)\",\n    fill = \"Purchaser Address Indicator\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\nClick to show code\n\n\ncounts &lt;- table(combined_ds$`Type of Sale`, combined_ds$`Property Type`)\npercentages &lt;- prop.table(counts, margin = 1) * 100 \n\ndf_percentages &lt;- as.data.frame(as.table(percentages))\n\nbarchart &lt;- ggplot(df_percentages, aes(x = Var1, y = Freq, fill = Var2)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Freq)), position = position_dodge(width = 0.9), vjust = -0.5, size = 2) +  \n  labs(title = \"Comparing Property Types Across Sale Categories\",\n       x = \"Sale Categories\",\n       y = \"Percentage (%)\", \n       fill = \"Property Type\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") \n\n\n\n\n\n\n\nClick to show code\n\n\n# Adjust text size for the bar chart\nbarchart1 &lt;- barchart1 + theme(\n  plot.title = element_text(size = 8),      # Adjust title size\n  axis.title = element_text(size = 8),       # Adjust axis titles size\n  axis.text.x = element_text(size = 6, angle = 45, hjust = 1), # Adjust x axis text size\n  axis.text.y = element_text(size = 6),      # Adjust y axis text size\n  legend.text = element_text(size = 6)       # Adjust legend text size\n)\n\n# Adjust text size for the heatmap\n# Adjust the heatmap to move the legend to the bottom\nheatmap1 &lt;- heatmap1 + theme(\n  plot.title = element_text(size = 8), \n  axis.text.x = element_text(size = 6, angle = 45, hjust = 1), # Adjust x axis text size\n  axis.text.y = element_text(size = 6), \n  legend.position = \"bottom\",\n  legend.text = element_text(size = 6, angle = 45, hjust = 1),       # Adjust legend text size if necessary\n  legend.title = element_text(size = 6)       # Adjust legend title size if necessary\n)\n\n# Adjusting theme settings for barchart\nbarchart &lt;- barchart + theme(\n  plot.title = element_text(size = 8),  # Smaller plot title\n  axis.title = element_text(size = 8),   # Smaller axis titles\n  axis.text = element_text(size = 6),    # Smaller axis text\n  legend.text = element_text(size = 6),  # Smaller legend text\n  legend.title = element_text(size = 6)  # Smaller legend title\n)\n\n# Adjusting theme settings for plot\nplot &lt;- plot + theme(\n  plot.title = element_text(size = 8),\n  axis.title = element_text(size = 8),\n  axis.text = element_text(size = 6),\n  legend.text = element_text(size = 6),\n  legend.title = element_text(size = 8)\n)\n\n# Now combine the plots using patchwork\ncombined_plot2 &lt;- (barchart | plot) +\n  plot_layout(widths = c(1, 1))  # Adjust the relative widths if necessary\n\n# Print the combined plot\nprint(combined_plot2)\n\n\n\n\n\n\n\n\n\nAccording to REALIS data dictionary, ‚ÄòPurchaser Address Indicator‚Äô refers to the type of residence (either HDB flat or private property) associated with the purchaser‚Äôs address as indicated in the caveat. It doesn‚Äôt necessarily imply ownership but rather reflects the type of housing the purchaser resides in. If this information is unavailable, it‚Äôs marked as ‚ÄòN.A‚Äô.\n\ncombined_plot &lt;- barchart1 + heatmap1 + \n  plot_layout(widths = c(0.1, 0.1)) # Adjust the width ratio as needed\nprint(combined_plot)\n\n\n\n\n\n\n\ncombined_plot2 &lt;- (barchart | plot) +\n  plot_layout(widths = c(1, 1.5)) \nprint(combined_plot2)\n\n\n\n\n\n\n\n\n\nüìà Insight: \n\nPrivate property sales surge in Jul 2023 due to several big launches on the market and developers sold 1412 units in July compared to 278 in June. Read more.\nCentral Region Apartments are the most sought after in the market according to the heatmap, followed by condominiums.\nIn the new sale market, apartments represent 61% of sales, while condominiums make up 24.5%. In contrast, the resale market sees condominiums accounting for 48% and apartments at 27.2%. Detached, semi-detached, and terrace houses are less prevalent in both markets. This is attributed to land scarcity and high construction costs imposed by the Singapore government. Furthermore, many property owners retain these types of properties due to their freehold tenure.\nIt‚Äôs notable that a significant majority of private property buyers (presumably those residing in these properties) live in private residences. Interestingly, executive condominiums have the highest proportion of purchasers with HDB addresses, accounting for 53.6%. This could be attributed to the attractive rental yields in Singapore. Consequently, more residents might be purchasing executive condominiums primarily as investment properties.\n\n\n\n\nEDA 2\nWe analyzed unit price ($PSF) across Singapore‚Äôs planning regions for 2023, using four quarterly charts to visualize regional price variations. The Central Region consistently showed the highest median PSF, indicating its premium status. In contrast, the North East and East Regions maintained stable PSF values, balancing affordability with location.\n\n\nClick to show code\n\n\nq4 &lt;- ggplot(ds2, aes(x = `Planning Region`, y = `Unit Price ($ PSF)`)) +\n  geom_boxplot() +\n  labs(title = \"Price per Square Foot by Planning Region - Oct 23 - Dec 23\",\n       x = \"Planning Region\",\n       y = \"Unit Price ($ PSF)\") +\n  theme(text = element_text(size = 6)) \n\n# Price per Square Foot by Planning Region in DS3\nq3 &lt;- ggplot(ds3, aes(x = `Planning Region`, y = `Unit Price ($ PSF)`)) +\n  geom_boxplot() +\n  labs(title = \"Price per Square Foot by Planning Region - Jul 23 - Sep 23\",\n       x = \"Planning Region\",\n       y = \"Unit Price ($ PSF)\") +\n  theme(text = element_text(size = 6)) \n\n# Price per Square Foot by Planning Region in DS4\nq2 &lt;- ggplot(ds4, aes(x = `Planning Region`, y = `Unit Price ($ PSF)`)) +\n  geom_boxplot() +\n  labs(title = \"Price per Square Foot by Planning Region - Apr 23 - Jun 23\",\n       x = \"Planning Region\",\n       y = \"Unit Price ($ PSF)\") +\n  theme(text = element_text(size = 6)) \n\n# Price per Square Foot by Planning Region in DS5\nq1 &lt;- ggplot(ds5, aes(x = `Planning Region`, y = `Unit Price ($ PSF)`)) +\n  geom_boxplot() +\n  labs(title = \"Price per Square Foot by Planning Region - Jan 23 - Mar 23\",\n       x = \"Planning Region\",\n       y = \"Unit Price ($ PSF)\") +\n  theme(text = element_text(size = 6)) \n\n\n\ncombined_plot &lt;- wrap_plots(q1, q2, q3, q4)\ncombined_plot\n\n\n\n\n\n\n\n\n\n\nClick to show code\n\n\ncombined_ds &lt;- bind_rows(\n  ds1 %&gt;% mutate(Data_Source = \"data/ds1.csv\"),\n  ds2 %&gt;% mutate(Data_Source = \"data/ds2.csv\"),\n  ds3 %&gt;% mutate(Data_Source = \"data/ds3.csv\"),\n  ds4 %&gt;% mutate(Data_Source = \"data/ds4.csv\"),\n  ds5 %&gt;% mutate(Data_Source = \"data/ds5.csv\"),\n)\n\nsummary_stats &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`) %&gt;%\n  summarise(\n    median_PSF = median(`Unit Price ($ PSF)`, na.rm = TRUE),\n    lower_quartile = quantile(`Unit Price ($ PSF)`, 0.25, na.rm = TRUE),\n    upper_quartile = quantile(`Unit Price ($ PSF)`, 0.75, na.rm = TRUE)\n  )\n\np &lt;- ggplot(combined_ds, aes(x = `Planning Region`, y = `Unit Price ($ PSF)`)) +\n  geom_boxplot() +\n  stat_summary(\n    fun = median,\n    geom = \"point\",\n    shape = 23,\n    size = 3,\n    color = \"blue\",\n    position = position_dodge(width = 0.75)\n  ) +\n  labs(\n    title = \"Distribution of Price per Square Foot by Planning Region - 2023 Q1-Q4\",\n    x = \"Planning Region\",\n    y = \"Unit Price ($ PSF)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_plotly &lt;- ggplotly(p, tooltip = c(\"Planning Region\", \"y\", \"Data_Source\", \"median_PSF\", \"lower_quartile\", \"upper_quartile\"))\n\np_plotly &lt;- plotly::ggplotly(p, tooltip = c(\"Planning Region\", \"y\", \"Data_Source\", \"median_PSF\", \"lower_quartile\", \"upper_quartile\"))\n\n\n\np_plotly\n\n\n\n\n\n\nüìà Insight:  Each region in Singapore offers a unique property market landscape catering to different buyer preferences and budgets.  The Central Region stands out as a high-end market, while the East offers premium properties at better values. The North East and North Regions provide a mix of affordability and mid-range options, whereas the West Region offers diversity without extreme price outliers.\n\n\n\nEDA 3\n\nlibrary(viridis)\n\nggplot(combined_ds, aes(x = `Unit Price ($ PSF)`, y = `Planning Region`, fill = `Planning Region`)) +\n  geom_density_ridges(scale = 3) +\n  scale_fill_viridis(discrete = TRUE) +\n  labs(title = \"Unit Price ($ PSF) Distribution by Planning Region\",\n       x = \"Unit Price ($ PSF)\",\n       y = \"Planning Region\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nüìà Insight:  - The Central Region‚Äôs peak is the most pronounced and shifted towards the high, indicating that there is a high concentration of properties with higher unit prices.  - The East Region‚Äôs pea is lower and more towards the middle of the x-axis compared to the Central Region. This implies that while the East Region has properties with moderate unit prices, it does not reach as high of a price point as frequently as the Central Region.  - North East and North Regions peaks are less sharp and positioned towards the lower end of the price scale, suggesting a more affordable housing options and a wider distribution of unit prices. North Region appears to have a slightly broader distribution than North East.  - West Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex_6.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex_6.html",
    "title": "In Class Exercise 6",
    "section": "",
    "text": "Exploring the King James Bible.\nWe will be using https://kgjerde.github.io/corporaexplorer/articles/bible.html for practice.\n\nlibrary(stringi)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(rvest)\nlibrary(corporaexplorer)\nlibrary(readr)\nlibrary(quanteda)\nlibrary(tidytext)\nlibrary(tidyverse)\n#library(textnets) ignore this as has to download from github\n\n\nbible &lt;- readr::read_lines(\"http://www.gutenberg.org/cache/epub/10/pg10.txt\")\n\n\n# Collapsing into one string.\nbible &lt;- paste(bible, collapse = \"\\n\")\n\n# Identifying the beginning and end of the Bible / stripping PJ metadata\n # (technique borrowed from https://quanteda.io/articles/pkgdown/replication/digital-humanities.html).\nstart_v &lt;- stri_locate_first_fixed(bible, \"The First Book of Moses: Called Genesis\")[1]\nend_v &lt;- stri_locate_last_fixed(bible, \"Amen.\")[2]\nbible &lt;- stri_sub(bible, start_v, end_v)\n\n# In the file, every book in the bible is preceded by five newlines,\n  # which we use to split our string into a vector where each element is a book.\nbooks &lt;- stri_split_regex(bible, \"\\n{5}\") %&gt;%\n    unlist %&gt;%\n    .[-40]  # Removing the heading \"The New Testament of the King James Bible\",\n              # which also was preceded by five newlines.\n\n# Because of the structure of the text in the file:\n  # Replacing double or more newlines with two newlines, and a single newline with space.\nbooks &lt;- str_replace_all(books, \"\\n{2,}\", \"NEW_PARAGRAPH\") %&gt;%\n    str_replace_all(\"\\n\", \" \") %&gt;%\n    str_replace_all(\"NEW_PARAGRAPH\", \"\\n\\n\")\nbooks &lt;- books[3:68]  # The two first elements are not books\n\n# Identifying new chapters within each book and split the text into chapters.\n# (The first characters in chapter 2 will e.g. be 2:1)\nchapters &lt;- str_replace_all(books, \"(\\\\d+:1 )\", \"NEW_CHAPTER\\\\1\") %&gt;%\n    stri_split_regex(\"NEW_CHAPTER\")\n\n# Removing the chapter headings from the text (we want them as metadata).\nchapters &lt;- lapply(chapters, function(x) x[-1])\n\nMetadata\n\n# We are not quite happy with the long book titles in the King James Bible,\n  # so we retrieve shorter versions from esv.org which will take up less\n  # space in the corpus map plot.\nbook_titles &lt;- read_html(\"https://www.esv.org/resources/esv-global-study-bible/list-of-abbreviations\") %&gt;%\n  html_nodes(\"td:nth-child(1)\") %&gt;%\n  html_text() %&gt;%\n  .[13:78]  # Removing irrelevant elements after manual inspection.\n\n# We add a column indicating whether a book belongs to the Old or New Testament,\n#   knowing that they contain respectively 39 and 27 books.\ntestament &lt;- c(rep(\"Old\", 39), rep(\"New\", 27))\n\nCreating data frame with text and metadata\n\n# Data frame with one book as one row.\nbible_df &lt;- tibble::tibble(Text = chapters,\n                           Book = book_titles,\n                           Testament = testament)\n\n# We want each chapter to be one row, but keep the metadata (book and which testament).\nbible_df &lt;- tidyr::unnest(bible_df, Text)\n\ncorporaexplorer When we first have a data frame with text and metadata, creating a ‚Äúcorporaexplorerobject‚Äù for exploration is very simple:\n\n# As this is a corpus which is not organised by date,\n  # we set `date_based_corpus` to `FALSE`.\n# Because we want to organise our exploration around the books in the Bible,\n  # we pass `\"Book\"` to the `grouping_variable` argument.\n# We specify which metadata columns we want to be displayed in the\n  # \"Document information\" tab, using the `columns_doc_info` argument.\nKJB &lt;- prepare_data(dataset = bible_df,\n                    date_based_corpus = FALSE,\n                    grouping_variable = \"Book\",\n                    columns_doc_info = c(\"Testament\", \"Book\"))\n\n\nclass(KJB) #have to be before explore\n\n[1] \"corporaexplorerobject\"\n\n\n\nexplore(KJB) #opens shiney app\n\nShiny applications not supported in static R Markdown documents\n\n\n\nExtra Notes: - This particular exercise is very useful for MC1 in VAST challenge. -"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/in-class_Ex5b.html",
    "href": "In-class_Ex/In-class_Ex05/in-class_Ex5b.html",
    "title": "In Class Exercise 5b",
    "section": "",
    "text": "We will use jsonlite package to read the files for JSON files. Vast challenge 2024‚Äôs dataset will be used for this in class exercise.\n\npacman::p_load(tidyverse, jsonlite)\n\n\n# Specify the path to your JSON file\nfile_path &lt;- \"data/mc3.json\"\n\n# Read the file as lines of text\njson_data &lt;- readLines(file_path, warn = FALSE)\njson_string &lt;- paste(json_data, collapse = \"\")\n# Replace 'NaN' with 'null' in the JSON string\njson_string &lt;- gsub(\"NaN\", \"null\", json_string)\n# Parse the JSON string\njson_list &lt;- fromJSON(json_string)\n\n# Now json_list is a usable R list that you can work with\n# Check the structure of the list\nstr(json_list)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 60520 obs. of  15 variables:\n  ..$ type             : chr [1:60520] \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" ...\n  ..$ country          : chr [1:60520] \"Uziland\" \"Mawalara\" \"Uzifrica\" \"Islavaragon\" ...\n  ..$ ProductServices  : chr [1:60520] \"Unknown\" \"Furniture and home accessories\" \"Food products\" \"Unknown\" ...\n  ..$ PointOfContact   : chr [1:60520] \"Rebecca Lewis\" \"Michael Lopez\" \"Steven Robertson\" \"Anthony Wyatt\" ...\n  ..$ HeadOfOrg        : chr [1:60520] \"√âmilie-Susan Benoit\" \"Honor√© Lemoine\" \"Jules Labb√©\" \"Dr. V√≠ctor Hurtado\" ...\n  ..$ founding_date    : chr [1:60520] \"1954-04-24T00:00:00\" \"2009-06-12T00:00:00\" \"2029-12-15T00:00:00\" \"1972-02-16T00:00:00\" ...\n  ..$ revenue          : num [1:60520] 5995 71767 0 0 4747 ...\n  ..$ TradeDescription : chr [1:60520] \"Unknown\" \"Abbott-Gomez is a leading manufacturer and supplier of high-quality furniture and home accessories, catering to\"| __truncated__ \"Abbott-Harrison is a leading manufacturer of high-quality food products, including baked goods, snacks, and bev\"| __truncated__ \"Unknown\" ...\n  ..$ _last_edited_by  : chr [1:60520] \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:60520] \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:60520] \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ id               : chr [1:60520] \"Abbott, Mcbride and Edwards\" \"Abbott-Gomez\" \"Abbott-Harrison\" \"Abbott-Ibarra\" ...\n  ..$ dob              : chr [1:60520] NA NA NA NA ...\n $ links     :'data.frame': 75817 obs. of  11 variables:\n  ..$ start_date       : chr [1:75817] \"2016-10-29T00:00:00\" \"2035-06-03T00:00:00\" \"2028-11-20T00:00:00\" \"2024-09-04T00:00:00\" ...\n  ..$ type             : chr [1:75817] \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" ...\n  ..$ _last_edited_by  : chr [1:75817] \"Pelagia Alethea Mordoch\" \"Niklaus Oberon\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:75817] \"Existing Corporate Structure Data\" \"Oceanus Corporations Monthly - Jun '35\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:75817] \"Automatic Import\" \"Manual Entry\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ source           : chr [1:75817] \"Avery Inc\" \"Berger-Hayes\" \"Bowers Group\" \"Bowman-Howe\" ...\n  ..$ target           : chr [1:75817] \"Allen, Nichols and Thompson\" \"Jensen, Morris and Downs\" \"Barnett Inc\" \"Bennett Ltd\" ...\n  ..$ key              : int [1:75817] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ end_date         : chr [1:75817] NA NA NA NA ...\n\n# Summary of the list for a quick overview\nsummary(json_list)\n\n           Length Class      Mode   \ndirected    1     -none-     logical\nmultigraph  1     -none-     logical\ngraph       0     -none-     list   \nnodes      15     data.frame list   \nlinks      11     data.frame list   \n\n# Example: Replacing null values in a specific field\njson_list$PointOfContact &lt;- ifelse(is.null(json_list$PointOfContact), \"No Contact\", json_list$PointOfContact)\nmc3_data &lt;- fromJSON(json_string)\n\n\nmc1_data &lt;-fromJSON(\"data/mc1.json\")\nmc2_data &lt;-fromJSON(\"data/mc2.json\")\n\n\n\n#graph data model\n#you may apply this to MC2, MC3 as well\n#refer to MC1 Data Description for the data meanings. \n#MC3 is bit corrupted please artificially fill in something before run \n\n\nsummary(mc1_data)\n\n           Length Class      Mode   \ndirected    1     -none-     logical\nmultigraph  1     -none-     logical\ngraph       0     -none-     list   \nnodes       4     data.frame list   \nlinks      10     data.frame list   \n\nsummary(mc2_data)\n\n           Length Class      Mode   \ndirected    1     -none-     logical\nmultigraph  1     -none-     logical\ngraph       0     -none-     list   \nnodes      20     data.frame list   \nlinks      17     data.frame list   \n\nsummary(mc3_data)\n\n           Length Class      Mode   \ndirected    1     -none-     logical\nmultigraph  1     -none-     logical\ngraph       0     -none-     list   \nnodes      15     data.frame list   \nlinks      11     data.frame list"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/in-class_Ex4.html",
    "href": "In-class_Ex/In-class_Ex04/in-class_Ex4.html",
    "title": "In Class Exercise 4",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggstatsplot)\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\nset.seed(1234)\n\n\ngghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'parametric',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\ngghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'np',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n    xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\np &lt;- gghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'np',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n    xlab = \"English scores\"\n)\n\n\nextract_stats(p)\n\n$subtitle_data\n# A tibble: 1 √ó 12\n  statistic  p.value method                    alternative effectsize       \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;       &lt;chr&gt;            \n1     38743 3.43e-16 Wilcoxon signed rank test two.sided   r (rank biserial)\n  estimate conf.level conf.low conf.high conf.method n.obs expression\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt; &lt;list&gt;    \n1    0.528       0.95    0.430     0.613 normal        322 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\nNULL\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\n\n  gghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'bayes',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2), \n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nggdotplotstats(\n  data = exam,\n  x = ENGLISH, \n  y = CLASS,\n  title = \"\",\n  xlab = \"\"\n  \n)\n\n\n\n\n\n\n\n\n\nexam_long &lt;- exam %&gt;%\n   pivot_longer(\n     cols = ENGLISH:SCIENCE,\n     names_to = \"SUBJECT\",\n     values_to = \"SCORES\") %&gt;%\n  filter(CLASS ==\"3A\")\n\n\nggwithinstats(\n    data = filter(exam_long,\n                SUBJECT %in%\n                  c(\"MATHS\", \"SCIENCE\")),\n  x    = SUBJECT,\n  y    = SCORES,\n  type = \"p\"\n)\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH, \n  marginal = TRUE, \n  label.var = ID,\n  label.expression = ENGLISH &gt;90 & MATHS  &gt; 90,\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/in-class_Ex4.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04/in-class_Ex4.html#getting-started",
    "title": "In Class Exercise 4",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggstatsplot)\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\nset.seed(1234)\n\n\ngghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'parametric',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\ngghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'np',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n    xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\np &lt;- gghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'np',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth \n= 2), \n    xlab = \"English scores\"\n)\n\n\nextract_stats(p)\n\n$subtitle_data\n# A tibble: 1 √ó 12\n  statistic  p.value method                    alternative effectsize       \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;       &lt;chr&gt;            \n1     38743 3.43e-16 Wilcoxon signed rank test two.sided   r (rank biserial)\n  estimate conf.level conf.low conf.high conf.method n.obs expression\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt; &lt;list&gt;    \n1    0.528       0.95    0.430     0.613 normal        322 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\nNULL\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\n\n  gghistostats(\n  data = exam,\n  x = ENGLISH, \n  type = 'bayes',\n  test.value = 60, \n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7), \n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2), \n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nggdotplotstats(\n  data = exam,\n  x = ENGLISH, \n  y = CLASS,\n  title = \"\",\n  xlab = \"\"\n  \n)\n\n\n\n\n\n\n\n\n\nexam_long &lt;- exam %&gt;%\n   pivot_longer(\n     cols = ENGLISH:SCIENCE,\n     names_to = \"SUBJECT\",\n     values_to = \"SCORES\") %&gt;%\n  filter(CLASS ==\"3A\")\n\n\nggwithinstats(\n    data = filter(exam_long,\n                SUBJECT %in%\n                  c(\"MATHS\", \"SCIENCE\")),\n  x    = SUBJECT,\n  y    = SCORES,\n  type = \"p\"\n)\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH, \n  marginal = TRUE, \n  label.var = ID,\n  label.expression = ENGLISH &gt;90 & MATHS  &gt; 90,\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "The first part of the lesson was to mingle with Tableau. We have published our first date with Tableau here. Moving on to R studio, we will be referring to Visualising Distributions read more here for this exercise.\n#Load Package\n\npacman::p_load(ggrepel, patchwork, ggthemes, \n               tidyverse, ggridges, ggdist,\n               colorspace) \n\n\nexam_data &lt;- read_csv(\"Exam_data.csv\")\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\nHistogram\nUsing the steps you have learned, build a histogram.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\ngeom_histogram(bins=30,            \n                 color=\"black\",      \n                 fill=\"light blue\") \n\n\n\n\n\n\n\n\n\n\nProbability Density Plot\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_density(\n     color = \"#1696d2\",\n     adjust = .54,\n     alpha = .6\n  )\n\n\n\n\n\n\n\n\nThe alternative design. (Missing median_eng because the class abruptly ended.)\n\nmedian_eng &lt;- median(exam_data$ENGLISH)\nmean_eng &lt;- mean(exam_data$ENGLISH)\nstd_eng &lt;- (exam_data$ENGLISH)\n\nggplot(exam_data,\n       aes(x= ENGLISH)) +\n  geom_density(\n    color = \"#1696d2\",\n    adjust = .65,\n    alpha = .6) +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = mean_eng,\n                sd = std_eng),\n    col = \"grey10\",\n    size = .8) +\n  \n  geom_vline(\n    aes(xintercept = mean_eng), \n    color=\"#4d5887\",\n    linewidth = .6,\n    linetype = \"dashed\") +\n  annotate(geom=\"text\", \n           x = mean_eng - 8,\n           y = 0.04, \n           label = paste0(\"Mean ENGLISH: \",round((mean_eng),2)),\n           color = \"#4d5887\") \n\n\n\n\n\n\n\n\n\n\nVisualising Distribution with Ridgeline Plot\nRead more here.\n\nggplot(exam_data, aes(x = ENGLISH, y = fct_relevel(CLASS, rev(unique(CLASS))))) +\n  geom_density_ridges() +\n  scale_y_discrete(labels = rev) + # This is to ensure the order of classes is from top to bottom\n  labs(title = \"Distribution of English Scores by Class\",\n       x = \"English Score\",\n       y = \"Class\") +\n  theme_ridges(grid = FALSE) \n\n\n\n\n\n\n\n\n\nridgeline1 &lt;- ggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\nridgeline2 &lt;- ggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\nridgeline3 &lt;- ggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\nridgeline4 &lt;- ggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\nprint(ridgeline1)\n\n\n\n\n\n\n\nprint(ridgeline2) \n\n\n\n\n\n\n\nprint(ridgeline3) \n\n\n\n\n\n\n\nprint(ridgeline4)\n\n\n\n\n\n\n\n\n\n\n‚òÅ Rainploud Plot\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "Getting Started\nIn the code chunk below, p_load() of pacman package is used to load tidyverse family of packages.\n\npacman::p_load(tidyverse)\n\n\nrealis &lt;-read_csv(\"data/realis2019.csv\")\n\n\nggplot(data = realis,\n       aes(x = `Unit Price ($ psm)`)) +\n  geom_histogram()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/in-class_ex03.html",
    "href": "In-class_Ex/In-class_Ex03/in-class_ex03.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "Using Tableau for this exercise üì°"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/in-class_Ex5.html",
    "href": "In-class_Ex/In-class_Ex05/in-class_Ex5.html",
    "title": "In Class Exercise 5",
    "section": "",
    "text": "pacman::p_load(tidyverse, readtext,\n               quanteda, tidytext)\n\n\ndata_folder &lt;- \"data/articles\"\n\n\n#text_data &lt;- readtext(paste0(\"data/articles\",\"/*\")), class mate found another\n#way of writing refer below\n\ntext_data &lt;- readtext(\"data/articles/*\")\n\n\ncorpus_text &lt;- corpus(text_data)\nsummary(corpus_text, 5)\n\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n\n\n\nusenet_words &lt;- text_data %&gt;% \n  unnest_tokens(word,text) %&gt;%\n  filter (str_detect(word,\"[a-z']$\"),\n          !word %in% stop_words$word)\n\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 √ó 3\n  word             n text     \n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ‚Ñπ 3,254 more rows\n\n\nYou can use stringr package to slice your data, tidyr which is mainly for transforming numerical data, you can use separate wider delim.\n\ntext_data_splitted &lt;- text_data %&gt;%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\",\"Y\"), \n                       too_few = \"align_end\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex6b.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex6b.html",
    "title": "In Class Exercise 6b",
    "section": "",
    "text": "MC 3 Kick Starter, dataset we are using is from last year 2023. The dataset is slightly different from 2024.\n\npacman::p_load(jsonlite, tidygraph,ggraph,\n               visNetwork, graphlayouts, ggforce, \n               skimr, tidytext, tidyverse)\n\n\nmc3_data &lt;- fromJSON(\"MC3_2023.json\")\n\n\nclass(mc3_data)\n\n[1] \"list\"\n\n\n\nmc3_edges &lt;-\nas_tibble(mc3_data$links) %&gt;%\n    distinct() %&gt;%\n    mutate(source = \nas.character(source),\n        target = \nas.character(target),\n        type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;% \n    summarise(weights = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  ungroup()\n\n\nmc3_nodes &lt;- as_tibble(mc3_data$nodes) %&gt;%\n  mutate(country = as.character(country),\n        id = as.character(id),\n        product_services = as.character(product_services),\n        revenue_omu = \nas.numeric(as.character(revenue_omu)),\n        type = as.character(type)) %&gt;%\n  select(id, country, type, revenue_omu,\nproduct_services)\n\n\nid1 &lt;- mc3_edges%&gt;%\n  select(source) %&gt;%\n  rename(id = source)\nid2 &lt;- mc3_edges %&gt;%\n  select(target) %&gt;%\n  rename(id= target)\nmc3_nodes1 &lt;- rbind(id1, id2) %&gt;%\n  distinct() %&gt;%\n  left_join(mc3_nodes,\n            unmatched = \"drop\")\n\n\nmc3_graph &lt;- tbl_graph(nodes = mc3_nodes1,\n                       edges = mc3_edges,\n                       directed = FALSE) %&gt;%\n  mutate(betweenness_centrality = \ncentrality_betweenness(),\n         closeness_centrality = \ncentrality_closeness())\n\n\nmc3_graph %&gt;%\n  filter(betweenness_centrality &gt;= 300000) %&gt;%\nggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha=0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality, \n    colors = \"lightblue\",\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n\n\n\nmc3_graph %&gt;%\n  filter(betweenness_centrality &gt;= 100000) %&gt;%\nggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha=0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality, \n    colors = \"lightblue\",\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()"
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "",
    "text": "For this take-home exercise, I will select one data visualization from the submissions of our classmates for Take-Home Exercise 1. My task is to critique the selected visualization with respect to clarity and aesthetics. Following the critique, I will prepare a sketch for an alternative design, utilizing the principles and best practices of data visualization as covered in Lessons 1 and 2. Finally, I will recreate the original design using tools from ggplot2, its extensions, and other packages in the tidyverse suite.\nThe graph that has chosen is from here. It is a geom_history made by our fellow classmate."
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html#positive-points",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html#positive-points",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "Positive Points",
    "text": "Positive Points\n\nThe graph effectively clarifies the comparison across various property types, making it easy for the audience to grasp differences at a glance.\nThe colour scheme is okay and overall it is a graph with straightforward interpretation without overwhelming the viewer."
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html#improvements",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html#improvements",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "Improvements",
    "text": "Improvements\n\nTo further explore the relationship between total price and unit price per square foot across different property types, consider employing a more detailed graph, such as a violin plot. This could provide a deeper understanding of data distribution and variance within each category.\nEnhancing the bar chart with data labels could significantly improve its readability. By directly displaying key information on the graph, viewers can quickly comprehend the data without needing to cross-reference with the axis. This step will streamline the presentation and make the information more accessible at a glance."
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html#load-packages",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html#load-packages",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "Load Packages",
    "text": "Load Packages\n\n\nClick to show code\n\n\npacman::p_load(tidyverse, scales, patchwork, ggtext, DT)\n\n\n\nImport Data SetsConverting DataConverting Dates\n\n\n\n\nClick to show code\n\n\nds1 &lt;- read_csv(\"data/ds1.csv\")\nds2 &lt;- read_csv(\"data/ds2.csv\")\nds3 &lt;- read_csv(\"data/ds3.csv\")\nds4 &lt;- read_csv(\"data/ds4.csv\")\nds5 &lt;- read_csv(\"data/ds5.csv\")\n\n\n\n\n\n\nClick to show code\n\n\nprepare_dataset &lt;- function(ds) {\n  colSums(is.na(ds))\n  ds &lt;- na.omit(ds)\n  \n  ds$`Type of Sale` &lt;- tolower(as.character(ds$`Type of Sale`))\n  ds$`Type of Sale` &lt;- ifelse(ds$`Type of Sale` %in% c(\"new sale\", \"resale\"), ds$`Type of Sale`, \"other\")\n  ds$`Type of Sale` &lt;- as.factor(ds$`Type of Sale`)\n  \n  ds$`Property Type` &lt;- as.factor(ds$`Property Type`)\n  \n  ds$`Transacted Price ($)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Transacted Price ($)`, perl = TRUE))\n  ds$`Area (SQFT)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Area (SQFT)`, perl = TRUE))\n  ds$`Unit Price ($ PSF)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Unit Price ($ PSF)`, perl = TRUE))\n  \n  return(ds)\n}\n\n# Apply the function to each dataset\nds1 &lt;- prepare_dataset(ds1)\nds2 &lt;- prepare_dataset(ds2)\nds3 &lt;- prepare_dataset(ds3)\nds4 &lt;- prepare_dataset(ds4)\nds5 &lt;- prepare_dataset(ds5)\n\n# Combine the datasets\ncombined_ds &lt;- rbind(ds1, ds2, ds3, ds4, ds5)\n\n\n\n\n\n\nClick to show code\n\n\n# Convert Sale Date to Date format\nds1$`Sale Date` &lt;- dmy(ds1$`Sale Date`)\nds2$`Sale Date` &lt;- dmy(ds2$`Sale Date`)\nds3$`Sale Date` &lt;- dmy(ds3$`Sale Date`)\nds4$`Sale Date` &lt;- dmy(ds4$`Sale Date`)\nds5$`Sale Date` &lt;- dmy(ds5$`Sale Date`)"
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html#my-sketch",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html#my-sketch",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "My Sketch",
    "text": "My Sketch\nWhat I have in mind:"
  },
  {
    "objectID": "Take-home/Take-home_ex2/Take_home_x02.html#changes-implemented",
    "href": "Take-home/Take-home_ex2/Take_home_x02.html#changes-implemented",
    "title": "Take Home Exercise 1 - Part 2",
    "section": "Changes Implemented",
    "text": "Changes Implemented\n\nTransitioned to a Violin Chart: Instead of using a stacked bar chart, I have switched to a violin chart to better visualize the unit price per square meter across different property types. This change offers a clearer representation of the data distribution and variance.\nInclusion of IQR (Interquartile Range): The violin chart now includes the Interquartile Range (IQR) to provide a more detailed statistical summary. Additionally, users can hover over specific areas of the chart to view the exact values, enhancing interactive data exploration.\nModified Representation of Property Types: Rather than distinguishing property types by colour within a single chart, each property type is now represented by its own individual violin chart. This alteration avoids colour overlap and simplifies the differentiation between types, making it easier for the audience to interpret the data visually.\n\n\n\nClick to show code\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\nggplot_object &lt;- ggplot(data = combined_ds, aes(x = `Property Type`, y = `Unit Price ($ PSF)`, fill = `Property Type`)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\", outlier.shape = NA, alpha = 0.5) +  # Overlay boxplot\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 3) +  # Add a red dot for the median\n  labs(title = \"Distribution of Unit Price by Property Type with IQR\",\n       x = \"Property Type\",\n       y = \"Unit Price ($ PSF)\") +\n  theme_minimal() +\n  theme(legend.title = element_text(size = 8),\n        legend.text = element_text(size = 8),\n        axis.text.x = element_text(size = 8))  \n\n\nplotly_object &lt;- ggplotly(ggplot_object, tooltip = c(\"y\", \"x\"))\n\n\n\nplotly_object"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<strong>ISSS608</strong>",
    "section": "",
    "text": "‚ÄúWe‚Äôre entering a new world in which data may be more important than software.‚Äù\n‚Äî Tim O‚ÄôReilly\n\n\nThe Journey üê≥\nWelcome to ISSS608, my journey through visual analytics and applications.\nGuided by Prof.¬†Kam Tin Seong at Singapore Management University‚Äôs Master of IT in Business.\n\n\n\nThe Takeaways ü•°\n\nMaster foundational concepts and methodologies of visual analytics for effective data analysis and presentation.\nDesign and implement advanced visual analytics systems to support data-driven decision-making across various data types.\nCreate analytical dashboards using commercial software and open-source tools.\nGain the skills to design and implement advanced visual analytics systems, supporting data-driven decision-making across various data types, including numerical, multivariate, time-series, geographical, and network data."
  }
]